{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.datasets import TextClassificationDataset\n",
    "from torchtext.vocab import Vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## read data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Reviews.csv')\n",
    "df = df[['Score', 'Text']]\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates('Text')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "   Score                                               Text\n0      5  I have bought several of the Vitality canned d...\n1      1  Product arrived labeled as Jumbo Salted Peanut...\n2      4  This is a confection that has been around a fe...\n3      2  If you are looking for the secret ingredient i...\n4      5  Great taffy at a great price.  There was a wid...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "count                                                393579\nunique                                               393579\ntop       Recently purchased several cans of this for in...\nfreq                                                      1\nName: Text, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].describe()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "array([5, 1, 4, 2, 3])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].unique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## build dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "labels, reviews = df['Score'].to_numpy(), df['Text'].to_numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "make splits"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "def split_data(arrays, ratios=(0.7, 0.2, 0.1)):\n",
    "    data_len = arrays[0].shape[0]\n",
    "    assert all(a.shape[0] == data_len for a in arrays[1:])\n",
    "    sizes = [r / sum(ratios) for r in ratios]\n",
    "    sizes = [int(s * data_len) for s in sizes[:-1]]\n",
    "    sizes.append(data_len - sum(sizes))\n",
    "    start = 0\n",
    "    finish = 0\n",
    "    splits = []\n",
    "    for s in sizes:\n",
    "        finish += s\n",
    "        splits.append([a[start:finish] for a in arrays])\n",
    "        start += s\n",
    "    return splits\n",
    "\n",
    "train, valid, test = split_data((labels, reviews), (0.7, 0.2, 0.1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-12-5ebf752c66cb>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'label'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'text'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m}\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_merge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mvalid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_merge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvalid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0mtest\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_merge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtest\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-12-5ebf752c66cb>\u001B[0m in \u001B[0;36mdata_merge\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mdata_merge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m{\u001B[0m\u001B[0;34m'label'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'text'\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mrow\u001B[0m\u001B[0;34m}\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrow\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mtrain\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdata_merge\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def data_merge(data):\n",
    "    labels, text = data\n",
    "    return [{'label': int(l), 'text': row} for l, row in zip(labels, text)]\n",
    "\n",
    "train = data_merge(train)\n",
    "valid = data_merge(valid)\n",
    "test = data_merge(test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "del df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "tokenize text in datasets, add bigrams"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "! python -m spacy download en"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: 100%|██████████| 275505/275505 [01:21<00:00, 3400.75it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_tokenize(data, tokenizer, lower, ngrams):\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "    for entry in tqdm.tqdm(data, 'lines', len(data)):\n",
    "        if lower:\n",
    "            entry['text'] = entry['text'].lower()\n",
    "        entry['text'] = tokenizer(entry['text'])\n",
    "        entry['text'] = list(ngrams_iterator(entry['text'], ngrams))\n",
    "    return data\n",
    "\n",
    "tokenizer = 'spacy'\n",
    "lower = True\n",
    "ngrams = 2\n",
    "\n",
    "train = data_tokenize(train, tokenizer, lower, ngrams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: 100%|██████████| 78715/78715 [00:23<00:00, 3420.01it/s]\n",
      "lines: 100%|██████████| 39359/39359 [00:11<00:00, 3367.05it/s]\n"
     ]
    }
   ],
   "source": [
    "valid = data_tokenize(valid, tokenizer, lower, ngrams)\n",
    "test = data_tokenize(test, tokenizer, lower, ngrams)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "save just in case"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "with open('train_tokenized.json', 'wt') as file:\n",
    "    json.dump(train, file)\n",
    "\n",
    "with open('valid_tokenized.json', 'wt') as file:\n",
    "    json.dump(valid, file)\n",
    "\n",
    "with open('test_tokenized.json', 'wt') as file:\n",
    "    json.dump(test, file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "create vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def build_vocab(data, max_size=10000):\n",
    "    counter = Counter()\n",
    "    for label, row in data:\n",
    "        counter.update(row)\n",
    "    return Vocab(counter, max_size)\n",
    "\n",
    "vocab = build_vocab(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "create torch datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-21-ea546147dff9>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mTextClassificationDataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvocab\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlabels\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0mtrain_dataset\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msetup_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-21-ea546147dff9>\u001B[0m in \u001B[0;36msetup_dataset\u001B[0;34m(data, vocab)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0msetup_dataset\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mvocab\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m     \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mzip\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m     text = [[torch.tensor(vocab[token]) for token in row]\n\u001B[1;32m      4\u001B[0m             for row in tqdm.tqdm(text, 'lines', len(labels))]\n\u001B[1;32m      5\u001B[0m     \u001B[0mlabels_dict\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m{\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m}\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}