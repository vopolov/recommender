{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.vocab import Vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Reviews.csv')\n",
    "df = df[['Score', 'Text']]\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates('Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   Score                                               Text\n0      5  I have bought several of the Vitality canned d...\n1      1  Product arrived labeled as Jumbo Salted Peanut...\n2      4  This is a confection that has been around a fe...\n3      2  If you are looking for the secret ingredient i...\n4      5  Great taffy at a great price.  There was a wid...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Score</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>I have bought several of the Vitality canned d...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4</td>\n      <td>This is a confection that has been around a fe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>If you are looking for the secret ingredient i...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>Great taffy at a great price.  There was a wid...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "count                                                393579\nunique                                               393579\ntop       This is the best cat litter box next to the Li...\nfreq                                                      1\nName: Text, dtype: object"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "array([5, 1, 4, 2, 3])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels, reviews = df['Score'].to_numpy(), df['Text'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "make splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(arrays, ratios=(0.7, 0.2, 0.1)):\n",
    "    data_len = arrays[0].shape[0]\n",
    "    assert all(a.shape[0] == data_len for a in arrays[1:])\n",
    "    sizes = [r / sum(ratios) for r in ratios]\n",
    "    sizes = [int(s * data_len) for s in sizes[:-1]]\n",
    "    sizes.append(data_len - sum(sizes))\n",
    "    start = 0\n",
    "    finish = 0\n",
    "    splits = []\n",
    "    for s in sizes:\n",
    "        finish += s\n",
    "        splits.append([a[start:finish] for a in arrays])\n",
    "        start += s\n",
    "    return splits\n",
    "\n",
    "train, valid, test = split_data((labels, reviews), (0.7, 0.2, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_merge(data):\n",
    "    labels, text = data\n",
    "    return [{'label': int(l), 'text': row} for l, row in zip(labels, text)]\n",
    "\n",
    "train = data_merge(train)\n",
    "valid = data_merge(valid)\n",
    "test = data_merge(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "tokenize text in datasets, add bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: 100%|██████████| 275505/275505 [01:24<00:00, 3243.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_tokenize(data, tokenizer, lower, ngrams, cache=True):\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "    for entry in tqdm.tqdm(data, 'lines', len(data)):\n",
    "        if lower:\n",
    "            entry['text'] = entry['text'].lower()\n",
    "        entry['text'] = tokenizer(entry['text'])\n",
    "        entry['text'] = list(ngrams_iterator(entry['text'], ngrams))\n",
    "    return data\n",
    "\n",
    "tokenizer = 'spacy'\n",
    "lower = True\n",
    "ngrams = 2\n",
    "\n",
    "train = data_tokenize(train, tokenizer, lower, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: 100%|██████████| 78715/78715 [00:24<00:00, 3154.55it/s]\n",
      "lines: 100%|██████████| 39359/39359 [00:12<00:00, 3069.20it/s]\n"
     ]
    }
   ],
   "source": [
    "valid = data_tokenize(valid, tokenizer, lower, ngrams)\n",
    "test = data_tokenize(test, tokenizer, lower, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275505/275505 [00:06<00:00, 45648.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_tokenized(data, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        f.writelines(json.dumps(l) + '\\n' for l in tqdm.tqdm(data))\n",
    "\n",
    "save_tokenized(train, 'train_tokenized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78715/78715 [00:01<00:00, 45678.30it/s]\n",
      "100%|██████████| 39359/39359 [00:00<00:00, 42899.17it/s]\n"
     ]
    }
   ],
   "source": [
    "save_tokenized(valid, 'valid_tokenized.json')\n",
    "save_tokenized(test, 'test_tokenized.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "load data if available"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275505/275505 [00:08<00:00, 33676.79it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_tokenized(filename):\n",
    "    with open(filename, 'rt') as f:\n",
    "        return [json.loads(l) for l in tqdm.tqdm(f.readlines())]\n",
    "\n",
    "try:\n",
    "    if train:\n",
    "        pass\n",
    "except NameError:\n",
    "    train = load_tokenized('train_tokenized.json')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275505/275505 [00:10<00:00, 25736.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(data,\n",
    "                max_size=30000,  # x 100 emb_dim = about 3M model parameters\n",
    "                ):\n",
    "    counter = Counter()\n",
    "    for entry in tqdm.tqdm(data):\n",
    "        counter.update(entry['text'])\n",
    "    return Vocab(counter, max_size)\n",
    "\n",
    "vocab = build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "create torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class JsonDataset(Dataset):\n",
    "    def __init__(self, filename, vocab):\n",
    "        self.filename = filename\n",
    "        self.vocab = vocab\n",
    "        self.label_dict = {i + 1: i for i in range(5)}\n",
    "        self.data_len = None\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        with open(self.filename, 'rt') as f:\n",
    "            for i, l in enumerate(f):\n",
    "                if i == index:\n",
    "                    break\n",
    "        line = json.loads(l)\n",
    "        label = line['label']\n",
    "        label = self.label_dict[label]\n",
    "        text = line['text']\n",
    "        text = torch.tensor(np.fromiter((vocab[token] for token in text),\n",
    "                                        dtype='int'))\n",
    "        return label, text\n",
    "\n",
    "    def __len__(self):\n",
    "        if not self.data_len:\n",
    "            with open(self.filename) as f:\n",
    "                for i, l in enumerate(f):\n",
    "                    pass\n",
    "            self.data_len = i + 1\n",
    "        return self.data_len\n",
    "\n",
    "train_dataset = JsonDataset('train_tokenized.json', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "valid_dataset = JsonDataset('valid_tokenized.json', vocab)\n",
    "test_dataset = JsonDataset('test_tokenized.json', vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "assert len(train_dataset) == len(train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "(4,\n tensor([    5,    23,   188,   509,    12,     3,     0,  1101,   168,    85,\n           309,     6,    23,   173,    41,    57,     9,    45,    12,    43,\n           272,     2,     3,    52,  1204,    68,    35,     7,  8561,    78,\n             7,  2457,   833,     6,     8,  1236,   134,     2,    18, 26061,\n            13,  6014,     6,   152,     0,    14,    52,   134,    78,    10,\n           213,     2,    73,  3763, 18810,  7085,    58,     0,     0, 28801,\n          1013, 11736,  4716,   641,  1355,  3619,  2450, 16407,   157, 19358,\n          5927,  3090,  2636,    99,   381,     0,     0,  1932,   474,     0,\n             0,  1422,     0,     0,  8115,   161,  3327,     0,  1193,   428,\n             0,     0,     0,     0,  1237,     0,     0,   137,     0,   496,\n             0,  5994, 12876]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def padded_collate(batch, padding):\n",
    "    labels, texts = zip(*batch)\n",
    "    labels = torch.tensor(labels)\n",
    "    texts = pad_sequence(texts, padding_value=padding)\n",
    "    return labels, texts\n",
    "\n",
    "padding = vocab['<pad>']\n",
    "collate = partial(padded_collate, padding=padding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_iter = DataLoader(train_dataset,\n",
    "                        batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate,\n",
    "                        num_workers=4)\n",
    "valid_iter = DataLoader(valid_dataset,\n",
    "                        batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate,\n",
    "                        num_workers=4)\n",
    "test_iter  = DataLoader(test_dataset,\n",
    "                        batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate,\n",
    "                        num_workers=4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_len, embed_dim, n_classes, padding):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_len, embed_dim, padding)\n",
    "        self.fc = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # shape = [seq_dim, batch_dim, embed_dim]\n",
    "        x = x.mean(0)\n",
    "        return self.fc(x)\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "embed_dim = 100\n",
    "n_classes = 5\n",
    "model = TextModel(vocab_len, embed_dim, n_classes, padding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "3000705"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}