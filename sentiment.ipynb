{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchtext.data.utils import get_tokenizer, ngrams_iterator\n",
    "from torchtext.vocab import Vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/Reviews.csv')\n",
    "df = df[['Score', 'Text']]\n",
    "df = df.dropna()\n",
    "df = df.drop_duplicates('Text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Score</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Score                                               Text\n",
       "0      5  I have bought several of the Vitality canned d...\n",
       "1      1  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2      4  This is a confection that has been around a fe...\n",
       "3      2  If you are looking for the secret ingredient i...\n",
       "4      5  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count                                                393579\n",
       "unique                                               393579\n",
       "top       This is the best cat litter box next to the Li...\n",
       "freq                                                      1\n",
       "Name: Text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 1, 4, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## build dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels, reviews = df['Score'].to_numpy(), df['Text'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## make splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def split_data(arrays, ratios=(0.7, 0.2, 0.1)):\n",
    "    data_len = arrays[0].shape[0]\n",
    "    assert all(a.shape[0] == data_len for a in arrays[1:])\n",
    "    sizes = [r / sum(ratios) for r in ratios]\n",
    "    sizes = [int(s * data_len) for s in sizes[:-1]]\n",
    "    sizes.append(data_len - sum(sizes))\n",
    "    start = 0\n",
    "    finish = 0\n",
    "    splits = []\n",
    "    for s in sizes:\n",
    "        finish += s\n",
    "        splits.append([a[start:finish] for a in arrays])\n",
    "        start += s\n",
    "    return splits\n",
    "\n",
    "train, valid, test = split_data((labels, reviews), (0.7, 0.2, 0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def data_merge(data):\n",
    "    labels, text = data\n",
    "    return [{'label': int(l), 'text': row} for l, row in zip(labels, text)]\n",
    "\n",
    "train = data_merge(train)\n",
    "valid = data_merge(valid)\n",
    "test = data_merge(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## tokenize text in datasets, add bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: 100%|██████████| 275505/275505 [01:24<00:00, 3243.43it/s]\n"
     ]
    }
   ],
   "source": [
    "def data_tokenize(data, tokenizer, lower, ngrams, cache=True):\n",
    "    tokenizer = get_tokenizer(tokenizer)\n",
    "    for entry in tqdm.tqdm(data, 'lines', len(data)):\n",
    "        if lower:\n",
    "            entry['text'] = entry['text'].lower()\n",
    "        entry['text'] = tokenizer(entry['text'])\n",
    "        entry['text'] = list(ngrams_iterator(entry['text'], ngrams))\n",
    "    return data\n",
    "\n",
    "tokenizer = 'spacy'\n",
    "lower = True\n",
    "ngrams = 2\n",
    "\n",
    "train = data_tokenize(train, tokenizer, lower, ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "lines: 100%|██████████| 78715/78715 [00:24<00:00, 3154.55it/s]\n",
      "lines: 100%|██████████| 39359/39359 [00:12<00:00, 3069.20it/s]\n"
     ]
    }
   ],
   "source": [
    "valid = data_tokenize(valid, tokenizer, lower, ngrams)\n",
    "test = data_tokenize(test, tokenizer, lower, ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## save all data to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275505/275505 [00:06<00:00, 45648.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def save_tokenized(data, filename):\n",
    "    with open(filename, 'wt') as f:\n",
    "        f.writelines(json.dumps(l) + '\\n' for l in tqdm.tqdm(data))\n",
    "\n",
    "save_tokenized(train, 'train_tokenized.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78715/78715 [00:01<00:00, 45678.30it/s]\n",
      "100%|██████████| 39359/39359 [00:00<00:00, 42899.17it/s]\n"
     ]
    }
   ],
   "source": [
    "save_tokenized(valid, 'valid_tokenized.json')\n",
    "save_tokenized(test, 'test_tokenized.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train data from json if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275505/275505 [00:07<00:00, 36462.82it/s]\n"
     ]
    }
   ],
   "source": [
    "def load_tokenized(filename):\n",
    "    with open(filename, 'rt') as f:\n",
    "        return [json.loads(l) for l in tqdm.tqdm(f.readlines())]\n",
    "\n",
    "try:\n",
    "    if train:\n",
    "        pass\n",
    "except NameError:\n",
    "    train = load_tokenized('train_tokenized.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## create vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 275505/275505 [00:09<00:00, 28322.22it/s]\n"
     ]
    }
   ],
   "source": [
    "def build_vocab(data,\n",
    "                max_size=30000,  # x 100 emb_dim = about 3M model parameters\n",
    "                ):\n",
    "    counter = Counter()\n",
    "    for entry in tqdm.tqdm(data):\n",
    "        counter.update(entry['text'])\n",
    "    return Vocab(counter, max_size)\n",
    "\n",
    "vocab = build_vocab(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## create torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading: 100%|██████████| 275505/275505 [00:27<00:00, 9908.80it/s] \n"
     ]
    }
   ],
   "source": [
    "class JsonDataset(Dataset):\n",
    "    def __init__(self, filename, vocab, preload):\n",
    "        self.filename = filename\n",
    "        self.vocab = vocab\n",
    "        self.label_dict = {i + 1: i for i in range(5)}\n",
    "        self.data_len = None\n",
    "        self.preload = preload\n",
    "        if self.preload:\n",
    "            self._preload_data()\n",
    "\n",
    "    def _preload_data(self):\n",
    "        with open(self.filename, 'rt') as f:\n",
    "            lines = [json.loads(l) for l in f.readlines()]\n",
    "        self.data = []\n",
    "        for l in tqdm.tqdm(lines, 'loading'):\n",
    "            label = l['label']\n",
    "            label = self.label_dict[label]\n",
    "            label = torch.tensor(label)\n",
    "            text = l['text']\n",
    "            text = torch.tensor(np.fromiter((vocab[token] for token in text),\n",
    "                                        dtype='int'))\n",
    "            self.data.append((label, text))\n",
    "        self.data_len = len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if self.preload:\n",
    "            return self.data[index]\n",
    "\n",
    "        with open(self.filename, 'rt') as f:\n",
    "            for i, l in enumerate(f):\n",
    "                if i == index:\n",
    "                    break\n",
    "        line = json.loads(l)\n",
    "        label = line['label']\n",
    "        label = self.label_dict[label]\n",
    "        label = torch.tensor(label)\n",
    "        text = line['text']\n",
    "        text = torch.tensor(np.fromiter((vocab[token] for token in text),\n",
    "                                        dtype='int'))\n",
    "        return label, text\n",
    "\n",
    "    def __len__(self):\n",
    "        if not self.data_len:\n",
    "            with open(self.filename) as f:\n",
    "                for i, l in enumerate(f):\n",
    "                    pass\n",
    "            self.data_len = i + 1\n",
    "        return self.data_len\n",
    "\n",
    "train_dataset = JsonDataset('train_tokenized.json', vocab, preload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading: 100%|██████████| 78715/78715 [00:07<00:00, 11154.39it/s]\n",
      "loading: 100%|██████████| 39359/39359 [00:03<00:00, 11154.46it/s]\n"
     ]
    }
   ],
   "source": [
    "valid_dataset = JsonDataset('valid_tokenized.json', vocab, True)\n",
    "test_dataset = JsonDataset('test_tokenized.json', vocab, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "assert len(train_dataset) == len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(4),\n",
       " tensor([    5,    23,   188,   509,    12,     3,     0,  1101,   168,    85,\n",
       "           309,     6,    23,   173,    41,    57,     9,    45,    12,    43,\n",
       "           272,     2,     3,    52,  1204,    68,    35,     7,  8561,    78,\n",
       "             7,  2457,   833,     6,     8,  1236,   134,     2,    18, 26061,\n",
       "            13,  6014,     6,   152,     0,    14,    52,   134,    78,    10,\n",
       "           213,     2,    73,  3763, 18810,  7085,    58,     0,     0, 28801,\n",
       "          1013, 11736,  4716,   641,  1355,  3619,  2450, 16407,   157, 19358,\n",
       "          5927,  3090,  2636,    99,   381,     0,     0,  1932,   474,     0,\n",
       "             0,  1422,     0,     0,  8115,   161,  3327,     0,  1193,   428,\n",
       "             0,     0,     0,     0,  1237,     0,     0,   137,     0,   496,\n",
       "             0,  5994, 12876]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## create dataloaders for padded sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def padded_collate(batch, padding):\n",
    "    labels, texts = zip(*batch)\n",
    "    labels = torch.tensor(labels)\n",
    "    texts = pad_sequence(texts, padding_value=padding)\n",
    "    return labels, texts\n",
    "\n",
    "padding = vocab['<pad>']\n",
    "collate = partial(padded_collate, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "train_iter = DataLoader(train_dataset,\n",
    "                        batch_size,\n",
    "                        shuffle=True,\n",
    "                        collate_fn=collate,\n",
    "                        num_workers=4)\n",
    "valid_iter = DataLoader(valid_dataset,\n",
    "                        batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate,\n",
    "                        num_workers=4)\n",
    "test_iter  = DataLoader(test_dataset,\n",
    "                        batch_size,\n",
    "                        shuffle=False,\n",
    "                        collate_fn=collate,\n",
    "                        num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## add fasttext model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self, vocab_len, embed_dim, n_classes, padding):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_len, embed_dim, padding)\n",
    "        self.fc = nn.Linear(embed_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        # shape = [seq_dim, batch_dim, embed_dim]\n",
    "        x = x.mean(0)\n",
    "        return self.fc(x)\n",
    "\n",
    "vocab_len = len(vocab)\n",
    "embed_dim = 100\n",
    "n_classes = 5\n",
    "model = TextModel(vocab_len, embed_dim, n_classes, padding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "number of parameters and output shape - sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number of learnable parameters: 3000705\n"
     ]
    }
   ],
   "source": [
    "print('total number of learnable parameters: {}'.format(\n",
    "      sum(p.numel() for p in model.parameters() if p.requires_grad)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model output shape with batch size 256: torch.Size([256, 5])\n"
     ]
    }
   ],
   "source": [
    "for l, t in train_iter:\n",
    "    break\n",
    "output = model(t)\n",
    "print('model output shape with batch size {}: {}'.format(\n",
    "    batch_size, output.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(train_iter, model, optimizer, criterion, device):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for labels, texts in tqdm.tqdm(train_iter, 'train_batch', leave=False, position=0):\n",
    "        labels, texts = labels.to(device), texts.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        preds = model(texts)\n",
    "        loss = criterion(preds, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += (preds.argmax(1) == labels).sum().item()\n",
    "        total += labels.shape[0]\n",
    "\n",
    "    return total_loss / total, total_acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(valid_iter, model, criterion, device):\n",
    "    total_loss = 0\n",
    "    total_acc = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for labels, texts in tqdm.tqdm(valid_iter, 'valid_batch', leave=False, position=0):\n",
    "            labels, texts = labels.to(device), texts.to(device)\n",
    "\n",
    "            preds = model(texts)\n",
    "            loss = criterion(preds, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_acc += (preds.argmax(1) == labels).sum().item()\n",
    "            total += labels.shape[0]\n",
    "\n",
    "    return total_loss / total, total_acc / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001\n",
      "Training loss    : 0.00467\n",
      "Training acc     : 0.63\n",
      "Validation loss  : 0.00382\n",
      "Validation acc   : 0.65\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002\n",
      "Training loss    : 0.00358\n",
      "Training acc     : 0.66\n",
      "Validation loss  : 0.00329\n",
      "Validation acc   : 0.69\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003\n",
      "Training loss    : 0.00320\n",
      "Training acc     : 0.69\n",
      "Validation loss  : 0.00306\n",
      "Validation acc   : 0.71\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004\n",
      "Training loss    : 0.00300\n",
      "Training acc     : 0.71\n",
      "Validation loss  : 0.00294\n",
      "Validation acc   : 0.72\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005\n",
      "Training loss    : 0.00288\n",
      "Training acc     : 0.72\n",
      "Validation loss  : 0.00286\n",
      "Validation acc   : 0.73\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 006\n",
      "Training loss    : 0.00279\n",
      "Training acc     : 0.73\n",
      "Validation loss  : 0.00282\n",
      "Validation acc   : 0.73\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 007\n",
      "Training loss    : 0.00272\n",
      "Training acc     : 0.74\n",
      "Validation loss  : 0.00279\n",
      "Validation acc   : 0.74\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 008\n",
      "Training loss    : 0.00268\n",
      "Training acc     : 0.74\n",
      "Validation loss  : 0.00277\n",
      "Validation acc   : 0.74\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 009\n",
      "Training loss    : 0.00263\n",
      "Training acc     : 0.75\n",
      "Validation loss  : 0.00275\n",
      "Validation acc   : 0.74\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 010\n",
      "Training loss    : 0.00259\n",
      "Training acc     : 0.75\n",
      "Validation loss  : 0.00274\n",
      "Validation acc   : 0.74\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 011\n",
      "Training loss    : 0.00255\n",
      "Training acc     : 0.75\n",
      "Validation loss  : 0.00273\n",
      "Validation acc   : 0.74\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 012\n",
      "Training loss    : 0.00251\n",
      "Training acc     : 0.76\n",
      "Validation loss  : 0.00273\n",
      "Validation acc   : 0.75\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 013\n",
      "Training loss    : 0.00248\n",
      "Training acc     : 0.76\n",
      "Validation loss  : 0.00273\n",
      "Validation acc   : 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:   0%|          | 0/1077 [00:00<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 014\n",
      "Training loss    : 0.00245\n",
      "Training acc     : 0.76\n",
      "Validation loss  : 0.00273\n",
      "Validation acc   : 0.75\n",
      "New best validation loss, saving model weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_batch:  69%|██████▊   | 739/1077 [00:11<00:05, 65.64it/s]"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "best_loss = None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(train_iter, model, optimizer, criterion, device)\n",
    "    valid_loss, valid_acc = evaluate(valid_iter, model, criterion, device)\n",
    "\n",
    "    print('Epoch: {:03d}'.format(epoch + 1))\n",
    "    print('Training loss    : {:.5f}'.format(train_loss))\n",
    "    print('Training acc     : {:.2f}'.format(train_acc))\n",
    "    print('Validation loss  : {:.5f}'.format(valid_loss))\n",
    "    print('Validation acc   : {:.2f}'.format(valid_acc))\n",
    "\n",
    "    if best_loss is None or valid_loss < best_loss:\n",
    "        print('New best validation loss, saving model weights')\n",
    "        best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'textmodel5-epoch{:02d}-loss{:.2e}.pt'.format(epoch, best_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
